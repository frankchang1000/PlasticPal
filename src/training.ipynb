{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"efficientnet_b0\"\n",
    "weights = None #\"imagenet\"\n",
    "input_dims = (32, 32, 3)\n",
    "num_classes = 100\n",
    "epochs = 25\n",
    "batch_size = 4\n",
    "shuffle_size = 16\n",
    "learning_rate = 0.001\n",
    "precision = \"float32\"\n",
    "model_dir = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'efficientnet_b0': tf.keras.applications.EfficientNetB0,\n",
    "    'efficientnet_b1': tf.keras.applications.EfficientNetB1,\n",
    "    'efficientnet_b2': tf.keras.applications.EfficientNetB2,\n",
    "    'efficientnet_b3': tf.keras.applications.EfficientNetB3,\n",
    "    'efficientnet_b4': tf.keras.applications.EfficientNetB4,\n",
    "    'efficientnet_b5': tf.keras.applications.EfficientNetB5,\n",
    "    'efficientnet_b6': tf.keras.applications.EfficientNetB6,\n",
    "    'efficientnet_b7': tf.keras.applications.EfficientNetB7\n",
    "}\n",
    "inputs = tf.keras.Input(shape=input_dims)\n",
    "backbone = models[model_name](include_top=True,\n",
    "                        weights=weights,\n",
    "                        input_shape=input_dims,\n",
    "                        classes=num_classes)(inputs)\n",
    "backbone.trainable = True\n",
    "# backbone = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(backbone)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=backbone, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"efficientnet_b0\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 100)              4177671   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,177,671\n",
      "Trainable params: 4,135,648\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import test training data (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_preprocess(image):\n",
    "    image = np.array(image)\n",
    "    transform = A.Compose([\n",
    "        A.RandomCrop(width=input_dims[0], height=input_dims[1]),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Resize(height=input_dims[0], width=input_dims[1]),\n",
    "    ])\n",
    "    transformed_image = transform(image=image)[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "def one_hot(indicies):\n",
    "    return tf.one_hot(indicies, num_classes)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ds = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "\n",
    "x_train_ds = x_train_ds.map(\n",
    "    lambda x: tf.numpy_function(\n",
    "        augment_preprocess,\n",
    "        inp=[x],\n",
    "        Tout=[tf.uint8]),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    name=\"augment\")\n",
    "\n",
    "y_train_ds = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "\n",
    "y_train_ds = y_train_ds.map(\n",
    "    lambda x: tf.numpy_function(\n",
    "        one_hot,\n",
    "        inp=[x],\n",
    "        Tout=[tf.float32]),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    name=\"one_hot\")\n",
    "train_dataset = tf.data.Dataset.zip((x_train_ds, y_train_ds)).shuffle(shuffle_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(4, 100), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.]], dtype=float32)>,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset:\n",
    "    #print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 0, Step: 0, Loss: 4.655250549316406\n",
      "Epoch: 0, Step: 1, Loss: 4.176300048828125\n",
      "Epoch: 0, Step: 2, Loss: 5.202820777893066\n",
      "Epoch: 0, Step: 3, Loss: 4.749520778656006\n",
      "Epoch: 0, Step: 4, Loss: 4.791799545288086\n",
      "Epoch: 0, Step: 5, Loss: 5.968967437744141\n",
      "Epoch: 0, Step: 6, Loss: 4.221220016479492\n",
      "Epoch: 0, Step: 7, Loss: 5.706993103027344\n",
      "Epoch: 0, Step: 8, Loss: 4.475464820861816\n",
      "Epoch: 0, Step: 9, Loss: 5.993929862976074\n",
      "Epoch: 0, Step: 10, Loss: 5.015353202819824\n",
      "Epoch: 0, Step: 11, Loss: 5.6083984375\n",
      "Epoch: 0, Step: 12, Loss: 6.529723644256592\n",
      "Epoch: 0, Step: 13, Loss: 6.094003200531006\n",
      "Epoch: 0, Step: 14, Loss: 6.47358512878418\n",
      "Epoch: 0, Step: 15, Loss: 5.533027648925781\n",
      "Epoch: 0, Step: 16, Loss: 4.177698612213135\n",
      "Epoch: 0, Step: 17, Loss: 5.612213611602783\n",
      "Epoch: 0, Step: 18, Loss: 5.180880069732666\n",
      "Epoch: 0, Step: 19, Loss: 7.220242500305176\n",
      "Epoch: 0, Step: 20, Loss: 6.132220268249512\n",
      "Epoch: 0, Step: 21, Loss: 5.882503509521484\n",
      "Epoch: 0, Step: 22, Loss: 6.198249816894531\n",
      "Epoch: 0, Step: 23, Loss: 4.205722332000732\n",
      "Epoch: 0, Step: 24, Loss: 4.111119270324707\n",
      "Epoch: 0, Step: 25, Loss: 5.060230255126953\n",
      "Epoch: 0, Step: 26, Loss: 6.231145858764648\n",
      "Epoch: 0, Step: 27, Loss: 6.9985809326171875\n",
      "Epoch: 0, Step: 28, Loss: 6.691983222961426\n",
      "Epoch: 0, Step: 29, Loss: 5.565163612365723\n",
      "Epoch: 0, Step: 30, Loss: 5.962871551513672\n",
      "Epoch: 0, Step: 31, Loss: 6.9694061279296875\n",
      "Epoch: 0, Step: 32, Loss: 6.8812971115112305\n",
      "Epoch: 0, Step: 33, Loss: 5.943663120269775\n",
      "Epoch: 0, Step: 34, Loss: 5.876996994018555\n",
      "Epoch: 0, Step: 35, Loss: 6.151147842407227\n",
      "Epoch: 0, Step: 36, Loss: 6.118171215057373\n",
      "Epoch: 0, Step: 37, Loss: 5.048439979553223\n",
      "Epoch: 0, Step: 38, Loss: 4.525914669036865\n",
      "Epoch: 0, Step: 39, Loss: 4.911240577697754\n",
      "Epoch: 0, Step: 40, Loss: 5.274062156677246\n",
      "Epoch: 0, Step: 41, Loss: 5.064254283905029\n",
      "Epoch: 0, Step: 42, Loss: 5.708392143249512\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "    for step, (x, y) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = model(x, training=True)\n",
    "            loss = loss_func(y_true=y[0], y_pred=out)\n",
    "            if precision == \"mixed_float16\":\n",
    "                loss = optimizer.get_scaled_loss(loss)\n",
    "        gradients = tape.gradient(\n",
    "            target=loss,\n",
    "            sources=model.trainable_variables)\n",
    "        if precision == \"mixed_float16\":\n",
    "            gradients = optimizer.get_unscaled_gradients(gradients)\n",
    "        optimizer.apply_gradients(\n",
    "            grads_and_vars=zip(gradients, model.trainable_variables))\n",
    "        if step % 100 == 0:\n",
    "            model.save_weights(f\"{model_dir}/{model_name}_weights_{step}.h5\")\n",
    "        print(\"Epoch: {}, Step: {}, Loss: {}\".format(epoch, step, loss.numpy()))\n",
    "\n",
    "# tf.keras.models.save_model(model, f\"{model_dir}/{model_name}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5b6d09c6aa7fff57d7a8369c5cfbd30b60cd54efea322b6d32834da73899050"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
